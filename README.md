# CNN Edge Quantization: Brevitas + FINN on Ultra96v2

## 🧠 What is this repository?

This project explores quantization-aware training and hardware synthesis to deploy CNNs for **classification and regression** on resource-constrained embedded FPGAs (Ultra96v2).  
The target application is a **light-based sensor** that determines surface suitability based on optical ring reflection, with models designed to either:
- classify if the image is reliable for traditional estimation,
- or regress the diameter of the reflection for alignment analysis.

The full pipeline uses **Brevitas** to train quantized models and **FINN** to compile them into synthesizable hardware architectures.

---

## 📂 Repository Structure

```bash
cnn-edge-quantization/
├── brevitasTraining/             # Training and dataset creation
│   ├── Classification/
│   │   ├── CreateDataset.ipynb   # Create train/test data for classification (ML + board)
│   │   ├── Size128_64/
│   │   │   └── Brevitas model training (2-class, small input)
│   │   └── Size300/
│   │       ├── Brevitas, PyTorch, and TF versions
│   ├── Regression/
│   │   ├── CreateDatasetBoard.ipynb  # Generates board test dataset
│   │   ├── Size64/128/300/
│   │   │   └── Regression model notebooks (dataset creation is embedded inside each notebook)
├── finnAccellerator/
│   └── advanced/
│       ├── *.ipynb               # Final FINN experiments (incl. FIFO tuning)
│       └── driver_example.py     # Example inference driver
└── README.md                     # (this file)
```

---

## ⚙️ How to Use

### 1. Train and Export ONNX

Inside `brevitasTraining/`:

- Run `CreateDataset.ipynb` (Classification) or `CreateDatasetBoard.ipynb` (Regression) to prepare test datasets for board testing.
- For **Classification**:
  - Dataset creation is done in the external script.
  - Each input size folder contains one or more Brevitas training notebooks. Train the model and export it to ONNX.
- For **Regression**:
  - Dataset creation for training is embedded in each notebook.
  - Use `CreateDatasetBoard.ipynb` to generate the dataset for final board evaluation.

💡 **Exported ONNX models** are the input to FINN.

---

### 2. Build with FINN

Inside `finnAccellerator/advanced/`:

- Use one of the provided `4_advanced_builder_settings_*.ipynb` notebooks.
- Each notebook corresponds to a trained Brevitas model (Classification or Regression), with embedded quantization and architectural tuning (PE=1, SIMD=1).
- Includes a special experiment for **fixed MAX FIFO** (did not work reliably on board).

💡 Use the final **output folder** generated by FINN to retrieve the `bitfile`.

---

### 3. Deploy on FPGA (Ultra96v2)

Modify `driver.py` for execution:
```python
from pynq import Device
...
device = Device.active_device
```

Then run:

#### 🧪 Inference
- **Classification**:
```bash
sudo -E python3 driver.py --exec_mode execute \
  --bitfile ../bitfile/finn-accel.bit \
  --inputfile /home/xilinx/pynq/finn_cnn/dataset/classification/test_input_600samples_reverse_255values_size128.npy \
  --outputfile result.npy --batchsize 600
```

- **Regression**:
```bash
sudo -E python3 driver.py --exec_mode execute \
  --bitfile ../bitfile/finn-accel.bit \
  --inputfile /home/xilinx/pynq/finn_cnn/dataset/regression/X_test_size128_400samples_255datasetNew_uint8.npy \
  --outputfile result.npy --batchsize 400
```

#### 🚀 Throughput test (Regression)
```bash
sudo -E python3 driver.py --exec_mode throughput_test \
  --bitfile ../bitfile/finn-accel.bit \
  --inputfile /home/xilinx/pynq/finn_cnn/dataset/regression/X_test_size64_400samples_255datasetNew_uint8.npy \
  --outputfile result.npy --batchsize 400
```

Alternative bitfile path:
```bash
sudo -E python3 driver.py --exec_mode throughput_test \
  --bitfile /usr/local/share/.../output_estimates_qonnx.../deploy/bitfile/finn-accel.bit \
  --inputfile /home/xilinx/pynq/finn_cnn/dataset/regression/X_test_size64_400samples_255datasetNew_uint8.npy \
  --outputfile result.npy --batchsize 400
```

---

## 🔧 Debugging Tips & Known Issues

- **ReLU** must be `QuantReLU` from Brevitas; standard ReLU will not be handled correctly in FINN.
- **BatchNorm** should NOT be quantized.
- **Softmax** must be excluded from hardware (leave outside the exported model).
- Avoid **quantization immediately after Flatten**—it may cause export/conversion issues.
- **Streamline and manage_gap** must be used correctly:  
  → `streamline → manage_gap → streamline`  
  to complete layer handling.
- **Transpose and Conv2D** are processed inside `streamline`.
- Use **Netron** to verify block removal and model structure.

### FIFO-related issues

- FINN tends to overestimate FIFO size on Ultra96v2.
- Even with `split_large_fifos=True` and fixed PE/SIMD=1, synthesis might fail.
- **Using only 1 neuron** in the last dense layer can cause:
  ```
  FileNotFoundError: .../verilator_fifosim.../results.txt
  ```
  ✅ Fixed by using 2 neurons instead.

- The **Max FIFO experiment**:
  - Only worked with `QuantIdentity`.
  - HW/SW mismatch persisted on board.

---

## 🧪 Best Working Configuration

From experiments:
- **Classification** is robust to quantization. High F1-score (~0.87) even at 64×64.
- **Regression** is more sensitive. Best trade-offs:
  - **Config A**: 64×64, 8-bit weights, filters 8–24 → 34.77 µm avg error
  - **Config B**: 64×64, 4-bit weights, filters 8–24 → 54.69 µm avg error

---

## 📌 Summary

This repo provides a complete CNN quantization and deployment pipeline on Ultra96v2 using Brevitas + FINN.  
It includes:
- Dataset creation
- Training (classification and regression)
- ONNX export
- Hardware-aware tuning
- FINN acceleration
- FPGA deployment and performance evaluation
