# CNN Edge Quantization: Brevitas + FINN on Ultra96v2

## ğŸ§  What is this repository?

This project explores quantization-aware training and hardware synthesis to deploy CNNs for **classification and regression** on resource-constrained embedded FPGAs (Ultra96v2).  
The target application is a **light-based sensor** that determines surface suitability based on optical ring reflection, with models designed to either:
- classify if the image is reliable for traditional estimation,
- or regress the diameter of the reflection for alignment analysis.

The full pipeline uses **Brevitas** to train quantized models and **FINN** to compile them into synthesizable hardware architectures.

---

## ğŸ“‚ Repository Structure

```bash
cnn-edge-quantization/
â”œâ”€â”€ brevitasTraining/             # Training and dataset creation
â”‚   â”œâ”€â”€ Classification/
â”‚   â”‚   â”œâ”€â”€ CreateDataset.ipynb   # Create train/test data for classification (ML + board)
â”‚   â”‚   â”œâ”€â”€ Size128_64/
â”‚   â”‚   â”‚   â””â”€â”€ Brevitas model training (2-class, small input)
â”‚   â”‚   â””â”€â”€ Size300/
â”‚   â”‚       â”œâ”€â”€ Brevitas, PyTorch, and TF versions
â”‚   â”œâ”€â”€ Regression/
â”‚   â”‚   â”œâ”€â”€ CreateDatasetBoard.ipynb  # Generates board test dataset
â”‚   â”‚   â”œâ”€â”€ Size64/128/300/
â”‚   â”‚   â”‚   â””â”€â”€ Regression model notebooks (dataset creation is embedded inside each notebook)
â”œâ”€â”€ finnAccellerator/
â”‚   â””â”€â”€ advanced/
â”‚       â”œâ”€â”€ *.ipynb               # Final FINN experiments (incl. FIFO tuning)
â”‚       â””â”€â”€ driver_example.py     # Example inference driver
â””â”€â”€ README.md                     # (this file)
```

---

## âš™ï¸ How to Use

### 1. Train and Export ONNX

Inside `brevitasTraining/`:

- Run `CreateDataset.ipynb` (Classification) or `CreateDatasetBoard.ipynb` (Regression) to prepare test datasets for board testing.
- For **Classification**:
  - Dataset creation is done in the external script.
  - Each input size folder contains one or more Brevitas training notebooks. Train the model and export it to ONNX.
- For **Regression**:
  - Dataset creation for training is embedded in each notebook.
  - Use `CreateDatasetBoard.ipynb` to generate the dataset for final board evaluation.

ğŸ’¡ **Exported ONNX models** are the input to FINN.

---

### 2. Build with FINN

Inside `finnAccellerator/advanced/`:

- Use one of the provided `4_advanced_builder_settings_*.ipynb` notebooks.
- Each notebook corresponds to a trained Brevitas model (Classification or Regression), with embedded quantization and architectural tuning (PE=1, SIMD=1).
- Includes a special experiment for **fixed MAX FIFO** (did not work reliably on board).

ğŸ’¡ Use the final **output folder** generated by FINN to retrieve the `bitfile`.

---

### 3. Deploy on FPGA (Ultra96v2)

Modify `driver.py` for execution:
```python
from pynq import Device
...
device = Device.active_device
```

Then run:

#### ğŸ§ª Inference
- **Classification**:
```bash
sudo -E python3 driver.py --exec_mode execute \
  --bitfile ../bitfile/finn-accel.bit \
  --inputfile /home/xilinx/pynq/finn_cnn/dataset/classification/test_input_600samples_reverse_255values_size128.npy \
  --outputfile result.npy --batchsize 600
```

- **Regression**:
```bash
sudo -E python3 driver.py --exec_mode execute \
  --bitfile ../bitfile/finn-accel.bit \
  --inputfile /home/xilinx/pynq/finn_cnn/dataset/regression/X_test_size128_400samples_255datasetNew_uint8.npy \
  --outputfile result.npy --batchsize 400
```

#### ğŸš€ Throughput test (Regression)
```bash
sudo -E python3 driver.py --exec_mode throughput_test \
  --bitfile ../bitfile/finn-accel.bit \
  --inputfile /home/xilinx/pynq/finn_cnn/dataset/regression/X_test_size64_400samples_255datasetNew_uint8.npy \
  --outputfile result.npy --batchsize 400
```

Alternative bitfile path:
```bash
sudo -E python3 driver.py --exec_mode throughput_test \
  --bitfile /usr/local/share/.../output_estimates_qonnx.../deploy/bitfile/finn-accel.bit \
  --inputfile /home/xilinx/pynq/finn_cnn/dataset/regression/X_test_size64_400samples_255datasetNew_uint8.npy \
  --outputfile result.npy --batchsize 400
```

---

## ğŸ”§ Debugging Tips & Known Issues

- **ReLU** must be `QuantReLU` from Brevitas; standard ReLU will not be handled correctly in FINN.
- **BatchNorm** should NOT be quantized.
- **Softmax** must be excluded from hardware (leave outside the exported model).
- Avoid **quantization immediately after Flatten**â€”it may cause export/conversion issues.
- **Streamline and manage_gap** must be used correctly:  
  â†’ `streamline â†’ manage_gap â†’ streamline`  
  to complete layer handling.
- **Transpose and Conv2D** are processed inside `streamline`.
- Use **Netron** to verify block removal and model structure.

### FIFO-related issues

- FINN tends to overestimate FIFO size on Ultra96v2.
- Even with `split_large_fifos=True` and fixed PE/SIMD=1, synthesis might fail.
- **Using only 1 neuron** in the last dense layer can cause:
  ```
  FileNotFoundError: .../verilator_fifosim.../results.txt
  ```
  âœ… Fixed by using 2 neurons instead.

- The **Max FIFO experiment**:
  - Only worked with `QuantIdentity`.
  - HW/SW mismatch persisted on board.

---

## ğŸ§ª Best Working Configuration

From experiments:
- **Classification** is robust to quantization. High F1-score (~0.87) even at 64Ã—64.
- **Regression** is more sensitive. Best trade-offs:
  - **Config A**: 64Ã—64, 8-bit weights, filters 8â€“24 â†’ 34.77 Âµm avg error
  - **Config B**: 64Ã—64, 4-bit weights, filters 8â€“24 â†’ 54.69 Âµm avg error

---

## ğŸ“Œ Summary

This repo provides a complete CNN quantization and deployment pipeline on Ultra96v2 using Brevitas + FINN.  
It includes:
- Dataset creation
- Training (classification and regression)
- ONNX export
- Hardware-aware tuning
- FINN acceleration
- FPGA deployment and performance evaluation
